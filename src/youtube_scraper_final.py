import pandas as pd
import re
import json
import os
from datetime import datetime, timedelta
import pytz
import time
import random
import yt_dlp
otros_canales=[ 'UCZ2V316UTXTyvd8w5wjA_Aw',
    'UCx6h-dWzJ5NpAlja1YsApdg',
    'UC8BxSGcBKriJvoeyKOnJ6tA',
    'UCCjG8NtOig0USdrT5D1FpxQ',
    'UCmgnsaQIK1IR808Ebde-ssA','UCPWXiRWZ29zrxPFIQT7eHSA',
    'UCqnbDFdCpuN8CMEg0VuEBqA','UCBi2mrWuNuyYy4gbM6fU18Q',
    'UCXIJgqnII2ZOINSWNOGFThA', 'UCeY0bbntWzzVIaj2z3QigXg',
    'UCw0_9Iih3qM_I_gFLrJeNRg', 'UC8p1vwvWtl6T73JiExfWs1g',
    'UCHd62-u_v4DvJ8TCFtpi4GA','UC7qZ_e097NBkgOljy1joVRA','UC2a35q7eyzkfoIusBzdH4Hw'
        'UCsCE4IMMFuwPYbwDqaz7udQ',
    'UCvAnclelY8eSq8GyPE19KTw', 'UCk2FZi3N0h8APcVBOisQCMQ',
    "UCmh7afBz-uWwOSSNTqUBAhg", ]
ap_key = "AIzaSyCnx9KNELdutZ4XJPZgWfFspQTnPmPj08M" # Esta clave no se usa con yt_dlp, pero se mantiene.
canales = [
'UCn4sPeUomNGIr26bElVdDYg'
]

# --- Funciones Auxiliares ---

def get_channel_name(channel_id):
    try:
        url = f"https://www.youtube.com/channel/{channel_id}/about"
        channel_id = channel_id.strip('"\'')
        ydl_opts = {
            #"cookiefile": "cookies.txt",
            "quiet": True,
            "skip_download": True,
            "format": "best",
            "no_warnings": True,
            "extract_flat": True,
            "ignoreerrors": True
        }
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            if info and "channel" in info and "uploader_id" in info:
                name = info["channel"]
                handle = info["uploader_id"]
                return name, handle
            else:
                print(f"‚ùå No se pudo obtener la informaci√≥n del canal {channel_id}.")
                return "desconocido", None
    except Exception as e:
        print(f"‚ùå Error al obtener info del canal {channel_id}: {e}")
        return "desconocido", None

def get_video_metadata(info):
    """Extrae vistas y likes de un diccionario de informaci√≥n de video."""
    return {
        'views': info.get('view_count'),
        'likes': info.get('like_count'),
    }

def get_comments(info):
    """Extrae comentarios de un diccionario de informaci√≥n de video."""
    comentarios = {}
    if info and "comments" in info:
        for comentario in info["comments"]:
            cid = comentario["id"]
            fecha = comentario["_time_text"]
            fecha = time_text_to_iso_argentina(fecha)
            comentarios[cid] = {
                "texto": comentario["text"],
                "fecha": fecha,
                "likes": comentario["like_count"],
                "autor": comentario["author"]
            }
    return comentarios

def time_text_to_iso_argentina(_time_text):
    tz = pytz.timezone("America/Argentina/Buenos_Aires")
    now = datetime.now(tz)
    match = re.match(r"(\d+)\s(\w+)\sago", _time_text)
    if not match:
        return None
    value, unit = match.groups()
    value = int(value)
    if unit.startswith("second"):
        delta = timedelta(seconds=value)
    elif unit.startswith("minute"):
        delta = timedelta(minutes=value)
    elif unit.startswith("hour"):
        delta = timedelta(hours=value)
    elif unit.startswith("day"):
        delta = timedelta(days=value)
    else:
        return None
    comment_time = now - delta
    return comment_time.strftime("%Y-%m-%dT%H:%M:%SZ")

# --- Funci√≥n principal para obtener videos ---

def obtener_videos_recientes(channel_videos_url,
                             videos_por_dia=10,
                             dias_atras=15,
                             max_videos_canal=300,
                             random_state=None):
    """
    Obtiene videos de un canal publicados en los √∫ltimos `dias_atras` d√≠as.
    Selecciona hasta `videos_por_dia` por cada d√≠a.
    """

    if random_state is not None:
        random.seed(random_state)

    argentina_tz = pytz.timezone("America/Argentina/Buenos_Aires")
    hoy = datetime.now(argentina_tz).date()
    fecha_inicio_periodo = hoy - timedelta(days=dias_atras)

    print(
        f"üìÖ Buscando videos desde {fecha_inicio_periodo} "
        f"hasta {hoy} ({dias_atras} d√≠as)..."
    )

    ydl_opts = {
        #"cookiefile":"cookies.txt",
        "format": "best",
        "quiet": True,
        "skip_download": True,
        "playlistend": max_videos_canal,
        "ignoreerrors": True,
        "no_warnings": True,
        "sleep_interval": 5,         
        "max_sleep_interval": 10, 
    }

    videos_por_fecha = {}

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(channel_videos_url, download=False)

        if not info or "entries" not in info:
            print("‚ö†Ô∏è No se pudieron obtener videos del canal.")
            return []

        for entry in info["entries"]:
            if not entry:
                continue

            # Saltar livestreams
            if entry.get("is_live"):
                continue

            upload_date = entry.get("upload_date")
            if not upload_date:
                continue

            try:
                fecha_video = datetime.strptime(upload_date, "%Y%m%d").date()
            except ValueError:
                continue

            # Filtrar por rango de fechas
            if not (fecha_inicio_periodo <= fecha_video <= hoy):
                continue

            video_id = entry.get("id")
            if not video_id:
                continue

            fecha_str = fecha_video.strftime("%Y-%m-%d")
            videos_por_fecha.setdefault(fecha_str, []).append(video_id)

    # Selecci√≥n final
    print("\nüìä Videos por d√≠a:")
    videos_seleccionados = []

    for fecha in sorted(videos_por_fecha.keys(), reverse=True):
        videos_del_dia = videos_por_fecha[fecha]
        print(f"  {fecha}: {len(videos_del_dia)} videos", end="")

        # Mantener orden cronol√≥gico
        videos_del_dia = list(reversed(videos_del_dia))

        if len(videos_del_dia) > videos_por_dia:
            seleccionados = random.sample(videos_del_dia, videos_por_dia)
            print(f" ‚Üí üé≤ {videos_por_dia} seleccionados")
        else:
            seleccionados = videos_del_dia
            print(" ‚Üí ‚úì todos")

        videos_seleccionados.extend(seleccionados)

    print(f"\n‚ú® Total de videos seleccionados: {len(videos_seleccionados)}\n")

    # Pausa leve para evitar rate limit
    time.sleep(random.uniform(1.5, 3.0))

    return videos_seleccionados


# --- Funci√≥n Principal (extraer_comentarios) ---

def extraer_comentarios(channel_ids=canales,
                        processed_file="processed_videos.json",
                        comments_dir="data_pablo",
                        actualizar_videos=True,
                        max_comentarios=50,
                        videos_por_dia=10,
                        dias_atras=15):
    """
    Extrae comentarios de videos publicados recientemente.
    
    Par√°metros:
    - channel_ids: Lista de IDs de canales
    - processed_file: Archivo con videos ya procesados
    - comments_dir: Directorio para guardar comentarios
    - actualizar_videos: Si actualizar videos ya procesados
    - max_comentarios: Cantidad m√°xima de comentarios por video (default: 50)
    - videos_por_dia: Cantidad de videos aleatorios por d√≠a (default: 10)
    - dias_atras: Cantidad de d√≠as hacia atr√°s (default: 15)
    """

    if os.path.exists(processed_file):
        with open(processed_file, "r", encoding="utf-8") as f:
            processed_videos = json.load(f)
            processed_videos = {ch: set(ids) for ch, ids in processed_videos.items()}
    else:
        processed_videos = {}

    for channel_id in channel_ids:
        print(f"\n{'='*80}")
        print(f"üéØ Procesando canal: {channel_id}")
        print(f"{'='*80}\n")
        
        channel_name, handle = get_channel_name(channel_id)
        if not handle:
            print(f"‚ö†Ô∏è Skipping channel {channel_id} due to missing handle.")
            continue
        
        url = f"https://www.youtube.com/{handle}/videos"

        canal_comments_file = os.path.join(comments_dir, f"comentarios_{channel_name}.json")
        if os.path.exists(canal_comments_file):
            with open(canal_comments_file, "r", encoding="utf-8") as f:
                canal_comments = json.load(f)
        else:
            canal_comments = {}
        
        updated_video_ids = set()
        
        # Asegurarse de que el canal_name est√© en processed_videos
        if channel_name not in processed_videos:
            processed_videos[channel_name] = set()

        videos_a_procesar_ahora = set()

        # A√±adir videos existentes que necesitan actualizaci√≥n
        if actualizar_videos:
            videos_a_procesar_ahora.update(processed_videos[channel_name])
            print(f"üîÑ Actualizando {len(processed_videos[channel_name])} videos existentes para {channel_name}...")

        # Obtener videos recientes (10 por d√≠a durante los √∫ltimos 15 d√≠as)
        print(f"\nüîç Obteniendo videos recientes para {channel_name}...")
        print(f"   Configuraci√≥n: {videos_por_dia} videos/d√≠a √ó {dias_atras} d√≠as")
        latest_video_ids = obtener_videos_recientes(url, videos_por_dia=videos_por_dia, dias_atras=dias_atras)
        print(f"üìä Total de videos seleccionados: {len(latest_video_ids)}\n")
        videos_a_procesar_ahora.update(latest_video_ids)

        contador_procesados = 0
        total_a_procesar = len(videos_a_procesar_ahora)
        
        for video_id in videos_a_procesar_ahora:
            contador_procesados += 1
            print(f"\n[{contador_procesados}/{total_a_procesar}] Procesando video: {video_id}")
            
            if not (video_id in canal_comments):
                # Pausa aleatoria antes de cada video para evitar rate limiting
                pausa = random.uniform(2.0, 5.0)
                print(f"‚è≥ Pausando {pausa:.2f} segundos...")
                time.sleep(pausa)

                # √önica llamada a yt_dlp.extract_info por video
                ydl_opts_video = {
                    #"cookiefile": "cookies.txt",
                    'quiet': True,
                    'skip_download': True,
                    "format": "best",
                    'no_warnings': True,
                    'extract_flat': False,
                    'getcomments': True,
                    'comment_limit': max_comentarios,
                    'ignoreerrors': True,
                    'force_json': False,
                    "sleep_interval": 5,          
                    "max_sleep_interval": 10, 
                }
                video_id = video_id.strip('"\'')
                video_url = f"https://www.youtube.com/watch?v={video_id}"
            
                info_dict = None
                try:
                    with yt_dlp.YoutubeDL(ydl_opts_video) as ydl:
                        info_dict = ydl.extract_info(video_url, download=False)
                except yt_dlp.DownloadError as e:
                    print(f"‚ùå Error al extraer info de {video_id}: {e}")

                # Si no se pudo obtener info, saltamos este video
                if not info_dict:
                    print(f"‚ö†Ô∏è No se pudo obtener informaci√≥n del video {video_id}. Saltando...")
                    continue

                # Verificaci√≥n adicional: confirmar que NO es un short
                duracion = info_dict.get("duration")
                if duracion and duracion < 61:
                    print(f"‚è≠Ô∏è Saltando {video_id}: detectado como Short (duraci√≥n: {duracion}s)")
                    continue

                updated_video_ids.add(video_id)

                stats = get_video_metadata(info_dict)
                metrics = {
                    "_metrics": {
                        "views": stats.get("views"),
                        "likes": stats.get("likes"),
                    }
                }
                nuevos_comentarios = get_comments(info_dict)

                historico_video = {
                    k: v for k, v in canal_comments.get(video_id, {}).items() if k != "_metrics"
                }

                for cid, data in nuevos_comentarios.items():
                    if cid not in historico_video:
                        historico_video[cid] = data

                canal_comments[video_id] = {**metrics, **historico_video}
                print(f"‚úÖ Video procesado: {len(nuevos_comentarios)} comentarios extra√≠dos")
        
        # Guardar comentarios actualizados por canal
        os.makedirs(comments_dir, exist_ok=True)
        with open(canal_comments_file, "w", encoding="utf-8") as f:
            json.dump(canal_comments, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ Guardados comentarios de {len(updated_video_ids)} videos para {channel_name}")
        
        # Actualizar processed_videos para este canal
        processed_videos[channel_name] = list(updated_video_ids)

    # Guardar processed_videos global
    with open(processed_file, "w", encoding="utf-8") as f:
        json.dump({ch: list(ids) for ch, ids in processed_videos.items()}, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*80}")
    print(f"‚ú® Proceso completado exitosamente")
    print(f"{'='*80}\n")

# --- Extrae usuarios por canal ---
def usuarios_canal(input_dir="comentarios_por_canal", output_dir="usuarios_por_canal"):
    os.makedirs(output_dir, exist_ok=True)

    for archivo in os.listdir(input_dir):
        if archivo.endswith(".json"):
            ruta = os.path.join(input_dir, archivo)

            with open(ruta, "r", encoding="utf-8") as f:
                data = json.load(f)

            canal_nombre_archivo = os.path.splitext(archivo)[0].replace("comentarios_", "")
            salida = os.path.join(output_dir, f"{canal_nombre_archivo}.json")

            if os.path.exists(salida):
                with open(salida, "r", encoding="utf-8") as f:
                    canal_data = json.load(f)
            else:
                canal_data = {}
            
            id_videos = [k for k in data.keys() if k != "_metrics"]

            for video in id_videos:
                usuarios = set(canal_data.get(video, []))

                comentarios_del_video = data[video].keys()
                for comentario_key in comentarios_del_video:
                    if comentario_key != "_metrics":
                        autor = data[video][comentario_key]["autor"]
                        usuarios.add(autor)

                canal_data[video] = list(usuarios)
            
            with open(salida, "w", encoding="utf-8") as f:
                json.dump(canal_data, f, ensure_ascii=False, indent=2)


# --- Ejecuci√≥n del script ---
if __name__ == "__main__":
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë     EXTRACTOR DE COMENTARIOS DE YOUTUBE - VERSI√ìN 2.3      ‚ïë
    ‚ïë                                                            ‚ïë
    ‚ïë  Configuraci√≥n:                                            ‚ïë
    ‚ïë  ‚Ä¢ 10 videos aleatorios por d√≠a                            ‚ïë
    ‚ïë  ‚Ä¢ Hasta 50 comentarios por video                          ‚ïë
    ‚ïë  ‚Ä¢ √öltimos 15 d√≠as                                         ‚ïë
    ‚ïë  ‚Ä¢ Solo videos regulares (NO shorts)                       ‚ïë
    ‚ïë  ‚Ä¢ Excluye livestreams                                     ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # Ejecutar extracci√≥n
    extraer_comentarios(
        channel_ids=canales,
        dias_atras=15,           # √öltimos 15 d√≠as
        videos_por_dia=10,       # 10 videos aleatorios por d√≠a
        max_comentarios=50,      # Hasta 50 comentarios por video
        actualizar_videos=True
    )
    
    print("\nüéâ ¬°Extracci√≥n completada!")